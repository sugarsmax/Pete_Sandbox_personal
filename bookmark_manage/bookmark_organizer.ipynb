{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48028fc6",
   "metadata": {},
   "source": [
    "# Chrome Bookmark Organizer\n",
    "\n",
    "This notebook analyzes and reorganizes Chrome bookmarks while:\n",
    "- Maintaining top-level folder structure\n",
    "- Identifying duplicate bookmarks\n",
    "- Flagging very long URLs\n",
    "- Handling bookmarks with missing titles\n",
    "\n",
    "## Usage\n",
    "1. Export Chrome bookmarks to HTML (Chrome -> Bookmarks -> Bookmark Manager -> ⋮ -> Export bookmarks)\n",
    "2. Place the exported file in this directory\n",
    "3. Run all cells in this notebook\n",
    "4. Review the analysis before applying any changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "823f9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "# Seaborn import removed since it's not installed\n",
    "from IPython.display import display, HTML\n",
    "from datetime import datetime\n",
    "\n",
    "# Constants\n",
    "BOOKMARK_FILE = 'bookmarks_20250825.html'\n",
    "LONG_URL_THRESHOLD = 100  # URLs longer than this will be flagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bookmarks(file_path):\n",
    "    \"\"\"\n",
    "    Parse Chrome bookmarks HTML file into a structured format.\n",
    "    Returns a tuple of (all_bookmarks, folder_structure)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Bookmark file not found: {file_path}\")\n",
    "        \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # Fix potential HTML issues\n",
    "    content = content.replace('&', '&amp;')\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    bookmarks = []\n",
    "    folder_structure = defaultdict(list)\n",
    "    current_folder = [\"root\"]\n",
    "    \n",
    "    def process_node(node, depth=0):\n",
    "        if not node:\n",
    "            return\n",
    "            \n",
    "        if isinstance(node, str):\n",
    "            return\n",
    "            \n",
    "        # Handle DL tags\n",
    "        if node.name == 'dl':\n",
    "            for item in node.find_all('dt', recursive=False):\n",
    "                process_node(item, depth)\n",
    "            return\n",
    "            \n",
    "        # Handle DT tags\n",
    "        if node.name == 'dt':\n",
    "            # Check for folder (H3 tag)\n",
    "            h3 = node.find('h3', recursive=False)\n",
    "            if h3:\n",
    "                folder_name = h3.text.strip()\n",
    "                if h3.get('personal_toolbar_folder') == 'true':\n",
    "                    folder_name = \"Bookmarks Bar\"\n",
    "                current_folder.append(folder_name)\n",
    "                \n",
    "                # Process folder contents\n",
    "                dl = node.find('dl')\n",
    "                if dl:\n",
    "                    process_node(dl, depth + 1)\n",
    "                current_folder.pop()\n",
    "                return\n",
    "                \n",
    "            # Check for bookmark (A tag)\n",
    "            a = node.find('a', recursive=False)\n",
    "            if a:\n",
    "                url = a.get('href', '')\n",
    "                if url.startswith('javascript:'):\n",
    "                    return\n",
    "                    \n",
    "                title = a.text.strip() or url\n",
    "                add_date = a.get('add_date', '')\n",
    "                if add_date:\n",
    "                    try:\n",
    "                        add_date = datetime.fromtimestamp(int(add_date)).isoformat()\n",
    "                    except:\n",
    "                        add_date = ''\n",
    "                \n",
    "                bookmark = {\n",
    "                    'title': title,\n",
    "                    'url': url,\n",
    "                    'folder_path': '/'.join(current_folder),\n",
    "                    'depth': depth,\n",
    "                    'add_date': add_date,\n",
    "                    'icon': a.get('icon', '')\n",
    "                }\n",
    "                bookmarks.append(bookmark)\n",
    "                folder_structure['/'.join(current_folder)].append(bookmark)\n",
    "                return\n",
    "                \n",
    "            # Process any nested DL\n",
    "            dl = node.find('dl')\n",
    "            if dl:\n",
    "                process_node(dl, depth)\n",
    "    \n",
    "    # Start processing from the root DL tag\n",
    "    root_dl = soup.find('dl')\n",
    "    if root_dl:\n",
    "        process_node(root_dl)\n",
    "    \n",
    "    if not bookmarks:\n",
    "        print(\"Warning: No bookmarks were found. This might indicate a parsing issue.\")\n",
    "        print(\"First 100 characters of file:\", content[:100])\n",
    "        \n",
    "    df = pd.DataFrame(bookmarks)\n",
    "    if len(df) == 0:\n",
    "        print(\"DataFrame Info:\")\n",
    "        print(df.info())\n",
    "        print(\"\\nDataFrame Columns:\", list(df.columns))\n",
    "        \n",
    "    return df, dict(folder_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the parser\n",
    "print(\"Testing bookmark parser...\")\n",
    "try:\n",
    "    df, folder_structure = parse_bookmarks(BOOKMARK_FILE)\n",
    "    print(f\"\\nFound {len(df)} bookmarks in {len(folder_structure)} folders\")\n",
    "    print(\"\\nSample of parsed bookmarks:\")\n",
    "    if len(df) > 0:\n",
    "        print(df[['title', 'url', 'folder_path']].head())\n",
    "    else:\n",
    "        print(\"No bookmarks found!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing bookmarks: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "864ca7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bookmarks(df, folder_structure):\n",
    "    \"\"\"\n",
    "    Analyze bookmarks for issues and patterns.\n",
    "    Returns a dictionary of analysis results.\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        'total_bookmarks': len(df),\n",
    "        'total_folders': len(folder_structure),\n",
    "        'empty_titles': df[df['title'] == df['url']].shape[0],\n",
    "        'long_urls': df[df['url'].str.len() > LONG_URL_THRESHOLD].shape[0],\n",
    "        'avg_depth': df['depth'].mean(),\n",
    "        'duplicates': {}\n",
    "    }\n",
    "    \n",
    "    # Find duplicate URLs\n",
    "    duplicates = df[df.duplicated(['url'], keep=False)].sort_values('url')\n",
    "    analysis['duplicate_urls'] = len(duplicates['url'].unique())\n",
    "    analysis['duplicate_bookmarks'] = len(duplicates)\n",
    "    \n",
    "    # Group duplicates for detailed review\n",
    "    if not duplicates.empty:\n",
    "        analysis['duplicates'] = duplicates.groupby('url').apply(\n",
    "            lambda x: x[['title', 'folder_path', 'add_date']].to_dict('records')\n",
    "        ).to_dict()\n",
    "    \n",
    "    # Analyze folder distribution\n",
    "    folder_counts = df.groupby('folder_path').size()\n",
    "    analysis['folder_distribution'] = folder_counts.to_dict()\n",
    "    analysis['largest_folders'] = folder_counts.nlargest(5).to_dict()\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def display_analysis(analysis, df):\n",
    "    \"\"\"\n",
    "    Create a formatted display of the bookmark analysis.\n",
    "    \"\"\"\n",
    "    html_output = [\n",
    "        \"<h2>Bookmark Analysis Summary</h2>\",\n",
    "        f\"<p>Total Bookmarks: {analysis['total_bookmarks']}</p>\",\n",
    "        f\"<p>Total Folders: {analysis['total_folders']}</p>\",\n",
    "        f\"<p>Bookmarks without titles: {analysis['empty_titles']}</p>\",\n",
    "        f\"<p>URLs longer than {LONG_URL_THRESHOLD} characters: {analysis['long_urls']}</p>\",\n",
    "        f\"<p>Average folder depth: {analysis['avg_depth']:.1f}</p>\",\n",
    "        \n",
    "        \"<h3>Largest Folders</h3>\",\n",
    "        \"<ul>\"\n",
    "    ]\n",
    "    \n",
    "    for folder, count in analysis['largest_folders'].items():\n",
    "        html_output.append(f\"<li>{folder}: {count} bookmarks</li>\")\n",
    "    \n",
    "    html_output.append(\"</ul>\")\n",
    "    \n",
    "    if analysis['duplicate_urls'] > 0:\n",
    "        html_output.extend([\n",
    "            \"<h3>Duplicate URLs</h3>\",\n",
    "            f\"<p>Found {analysis['duplicate_urls']} URLs duplicated across {analysis['duplicate_bookmarks']} bookmarks</p>\",\n",
    "            \"<table border='1'>\",\n",
    "            \"<tr><th>URL</th><th>Occurrences</th><th>Locations</th></tr>\"\n",
    "        ])\n",
    "        \n",
    "        for url, instances in analysis['duplicates'].items():\n",
    "            locations = [f\"{b['folder_path']} ({b['title']})\" for b in instances]\n",
    "            html_output.append(\n",
    "                f\"<tr><td>{url[:50]}{'...' if len(url) > 50 else ''}</td>\"\n",
    "                f\"<td>{len(instances)}</td>\"\n",
    "                f\"<td>{' | '.join(locations)}</td></tr>\"\n",
    "            )\n",
    "        \n",
    "        html_output.append(\"</table>\")\n",
    "    \n",
    "    # Display long URLs\n",
    "    if analysis['long_urls'] > 0:\n",
    "        html_output.extend([\n",
    "            \"<h3>Long URLs</h3>\",\n",
    "            \"<table border='1'>\",\n",
    "            \"<tr><th>Title</th><th>URL Length</th><th>URL</th></tr>\"\n",
    "        ])\n",
    "        \n",
    "        long_urls = df[df['url'].str.len() > LONG_URL_THRESHOLD]\n",
    "        for _, row in long_urls.iterrows():\n",
    "            html_output.append(\n",
    "                f\"<tr><td>{row['title']}</td>\"\n",
    "                f\"<td>{len(row['url'])}</td>\"\n",
    "                f\"<td>{row['url'][:50]}...</td></tr>\"\n",
    "            )\n",
    "        \n",
    "        html_output.append(\"</table>\")\n",
    "    \n",
    "    return HTML(''.join(html_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee4b7bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read bookmark file...\n",
      "First 500 characters of file:\n",
      "<!DOCTYPE NETSCAPE-Bookmark-file-1>\n",
      "<!-- This is an automatically generated file.\n",
      "     It will be read and overwritten.\n",
      "     DO NOT EDIT! -->\n",
      "<META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=UTF-8\">\n",
      "<TITLE>Bookmarks</TITLE>\n",
      "<H1>Bookmarks</H1>\n",
      "<DL><p>\n",
      "    <DT><H3 ADD_DATE=\"1706913652\" LAST_MODIFIED=\"1755640208\" PERSONAL_TOOLBAR_FOLDER=\"true\">Bookmarks Bar</H3>\n",
      "    <DL><p>\n",
      "        <DT><H3 ADD_DATE=\"1603242345\" LAST_MODIFIED=\"1749235301\">ADSK</H3>\n",
      "        <DL><p>\n",
      "            <DT><A HREF=\n",
      "...\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n",
      "None\n",
      "\n",
      "DataFrame Columns: []\n",
      "\n",
      "First few rows:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Error: Missing required columns: title, url, folder_path\n",
      "\n",
      "The bookmark file structure is unexpected. Please check the parsing logic.\n",
      "\n",
      "Expected columns: title, url, folder_path\n",
      "\n",
      "Please ensure the parse_bookmarks() function is correctly extracting these fields.\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze bookmarks\n",
    "try:\n",
    "    # Print the contents of the bookmark file for debugging\n",
    "    print(\"Attempting to read bookmark file...\")\n",
    "    with open(BOOKMARK_FILE, 'r', encoding='utf-8') as f:\n",
    "        print(f\"First 500 characters of file:\\n{f.read(500)}\\n...\")\n",
    "    \n",
    "    df, folder_structure = parse_bookmarks(BOOKMARK_FILE)\n",
    "    \n",
    "    # Debug dataframe contents\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nDataFrame Columns:\", df.columns.tolist())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Verify required columns exist before analysis\n",
    "    required_cols = ['title', 'url', 'folder_path']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_cols)}\")\n",
    "        \n",
    "    analysis = analyze_bookmarks(df, folder_structure)\n",
    "    display(display_analysis(analysis, df))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nPlease export your Chrome bookmarks and place the file in this directory.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nThe bookmark file structure is unexpected. Please check the parsing logic.\")\n",
    "    print(\"\\nExpected columns: title, url, folder_path\")\n",
    "    print(\"\\nPlease ensure the parse_bookmarks() function is correctly extracting these fields.\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    print(\"\\nPlease ensure you exported bookmarks correctly from Chrome:\")\n",
    "    print(\"Chrome -> Bookmarks -> Bookmark Manager -> ⋮ -> Export bookmarks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b870c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_folder_structure(df, folder_structure):\n",
    "    \"\"\"\n",
    "    Create visualizations of the bookmark folder structure\n",
    "    \"\"\"\n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn')\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. Folder size distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    folder_sizes = df.groupby('folder_path').size()\n",
    "    sns.histplot(folder_sizes, bins=20)\n",
    "    plt.title('Folder Size Distribution')\n",
    "    plt.xlabel('Number of Bookmarks')\n",
    "    plt.ylabel('Number of Folders')\n",
    "    \n",
    "    # 2. Bookmark depth distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.histplot(df['depth'], bins=range(df['depth'].max() + 2))\n",
    "    plt.title('Bookmark Depth Distribution')\n",
    "    plt.xlabel('Folder Depth')\n",
    "    plt.ylabel('Number of Bookmarks')\n",
    "    \n",
    "    # 3. URL length distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    url_lengths = df['url'].str.len()\n",
    "    sns.histplot(url_lengths, bins=30)\n",
    "    plt.axvline(x=LONG_URL_THRESHOLD, color='r', linestyle='--', label=f'Long URL threshold ({LONG_URL_THRESHOLD})')\n",
    "    plt.title('URL Length Distribution')\n",
    "    plt.xlabel('URL Length (characters)')\n",
    "    plt.ylabel('Number of Bookmarks')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 4. Top-level folder distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    top_level_folders = df['folder_path'].apply(lambda x: x.split('/')[1] if len(x.split('/')) > 1 else 'root')\n",
    "    top_folder_counts = top_level_folders.value_counts()\n",
    "    plt.pie(top_folder_counts.values, labels=top_folder_counts.index, autopct='%1.1f%%')\n",
    "    plt.title('Top-level Folder Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create and display visualizations if bookmarks are loaded\n",
    "try:\n",
    "    fig = visualize_folder_structure(df, folder_structure)\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"Please load bookmarks first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_reorganization(df, folder_structure):\n",
    "    \"\"\"\n",
    "    Suggest reorganization of bookmarks while preserving top-level structure.\n",
    "    Returns a dictionary of suggested changes.\n",
    "    \"\"\"\n",
    "    suggestions = {\n",
    "        'duplicates_to_remove': [],\n",
    "        'moves': [],\n",
    "        'long_urls_to_review': []\n",
    "    }\n",
    "    \n",
    "    # Handle duplicates\n",
    "    duplicates = df[df.duplicated(['url'], keep=False)].sort_values(['url', 'add_date'])\n",
    "    for url in duplicates['url'].unique():\n",
    "        dupes = duplicates[duplicates['url'] == url]\n",
    "        # Keep the newest bookmark in its current location\n",
    "        keep_idx = dupes['add_date'].idxmax()\n",
    "        remove_idx = dupes.index.difference([keep_idx])\n",
    "        \n",
    "        for idx in remove_idx:\n",
    "            suggestions['duplicates_to_remove'].append({\n",
    "                'url': dupes.loc[idx, 'url'],\n",
    "                'from_folder': dupes.loc[idx, 'folder_path'],\n",
    "                'keep_in': dupes.loc[keep_idx, 'folder_path']\n",
    "            })\n",
    "    \n",
    "    # Suggest moves for bookmarks in deep folders\n",
    "    deep_bookmarks = df[df['depth'] > 3]  # Suggest reorganizing bookmarks deeper than 3 levels\n",
    "    for _, bookmark in deep_bookmarks.iterrows():\n",
    "        folder_parts = bookmark['folder_path'].split('/')\n",
    "        if len(folder_parts) > 2:  # Has at least a top-level folder\n",
    "            top_level = folder_parts[1]\n",
    "            suggested_folder = f\"root/{top_level}\"\n",
    "            \n",
    "            suggestions['moves'].append({\n",
    "                'url': bookmark['url'],\n",
    "                'title': bookmark['title'],\n",
    "                'from_folder': bookmark['folder_path'],\n",
    "                'to_folder': suggested_folder,\n",
    "                'reason': 'Reduce folder depth'\n",
    "            })\n",
    "    \n",
    "    # Flag very long URLs for review\n",
    "    long_urls = df[df['url'].str.len() > LONG_URL_THRESHOLD]\n",
    "    for _, bookmark in long_urls.iterrows():\n",
    "        suggestions['long_urls_to_review'].append({\n",
    "            'url': bookmark['url'],\n",
    "            'title': bookmark['title'],\n",
    "            'folder': bookmark['folder_path'],\n",
    "            'length': len(bookmark['url'])\n",
    "        })\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "def display_suggestions(suggestions):\n",
    "    \"\"\"\n",
    "    Display reorganization suggestions in a formatted way\n",
    "    \"\"\"\n",
    "    html_output = [\"<h2>Suggested Reorganization</h2>\"]\n",
    "    \n",
    "    if suggestions['duplicates_to_remove']:\n",
    "        html_output.extend([\n",
    "            \"<h3>Duplicate Bookmarks to Remove</h3>\",\n",
    "            \"<table border='1'>\",\n",
    "            \"<tr><th>URL</th><th>Remove from</th><th>Keep in</th></tr>\"\n",
    "        ])\n",
    "        \n",
    "        for dup in suggestions['duplicates_to_remove']:\n",
    "            html_output.append(\n",
    "                f\"<tr><td>{dup['url'][:50]}...</td>\"\n",
    "                f\"<td>{dup['from_folder']}</td>\"\n",
    "                f\"<td>{dup['keep_in']}</td></tr>\"\n",
    "            )\n",
    "        html_output.append(\"</table>\")\n",
    "    \n",
    "    if suggestions['moves']:\n",
    "        html_output.extend([\n",
    "            \"<h3>Suggested Moves</h3>\",\n",
    "            \"<table border='1'>\",\n",
    "            \"<tr><th>Title</th><th>From Folder</th><th>To Folder</th><th>Reason</th></tr>\"\n",
    "        ])\n",
    "        \n",
    "        for move in suggestions['moves']:\n",
    "            html_output.append(\n",
    "                f\"<tr><td>{move['title']}</td>\"\n",
    "                f\"<td>{move['from_folder']}</td>\"\n",
    "                f\"<td>{move['to_folder']}</td>\"\n",
    "                f\"<td>{move['reason']}</td></tr>\"\n",
    "            )\n",
    "        html_output.append(\"</table>\")\n",
    "    \n",
    "    if suggestions['long_urls_to_review']:\n",
    "        html_output.extend([\n",
    "            \"<h3>Long URLs to Review</h3>\",\n",
    "            \"<table border='1'>\",\n",
    "            \"<tr><th>Title</th><th>Folder</th><th>URL Length</th><th>URL</th></tr>\"\n",
    "        ])\n",
    "        \n",
    "        for url in suggestions['long_urls_to_review']:\n",
    "            html_output.append(\n",
    "                f\"<tr><td>{url['title']}</td>\"\n",
    "                f\"<td>{url['folder']}</td>\"\n",
    "                f\"<td>{url['length']}</td>\"\n",
    "                f\"<td>{url['url'][:50]}...</td></tr>\"\n",
    "            )\n",
    "        html_output.append(\"</table>\")\n",
    "    \n",
    "    return HTML(''.join(html_output))\n",
    "\n",
    "# Generate and display reorganization suggestions\n",
    "try:\n",
    "    suggestions = suggest_reorganization(df, folder_structure)\n",
    "    display(display_suggestions(suggestions))\n",
    "except NameError:\n",
    "    print(\"Please load bookmarks first\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (venv)",
   "language": "python",
   "name": "venv312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
